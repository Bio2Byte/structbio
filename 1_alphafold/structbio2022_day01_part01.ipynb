{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c05c7e1-2830-4a7b-ac9e-55b245752562",
   "metadata": {
    "tags": []
   },
   "source": [
    "Structural biology - Practical day 01\n",
    "=======================================\n",
    "\n",
    "Part 01\n",
    "-------\n",
    "\n",
    "Document written by [AdriÃ¡n Diaz](mailto:adrian.diaz@vub.be) & [David Bickel](mailto:david.bickel@vub.be)\n",
    "\n",
    "**Vrije Universiteit Brussel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caa3d3a-1485-4534-80f4-276b851954b7",
   "metadata": {},
   "source": [
    "## The scenario\n",
    "\n",
    "We have these proteins involved: \n",
    "\n",
    "- Gyrase (dimer) `Gyr:Gyr`\n",
    "- Toxin (dimer) `CcdB:CcdB`\n",
    "- Anti-toxin `CcdA`\n",
    "\n",
    "### Sequences\n",
    "\n",
    "The following code block contains the residues of both Toxin and Anti-toxin proteins in FASTA format. You will use them to create the prediction job.\n",
    "\n",
    "```fasta\n",
    "> CcdA\n",
    "MKQRITVTVDSDSYQLLKAYDVNISGLVSTTMQNEARRLRAERWKAENQEGMAEVARFIEMNGSFADENRDW\n",
    "\n",
    "> CcdB\n",
    "MQFKVYTYKRESRYRLFVDVQSDIIDTPGRRMVIPLASARLLSDKVSRELYPVVHIGDESWRMMTTDMASVPVSVIGEEVADLSHRENDIKNAINLMFWGI\n",
    "```\n",
    "\n",
    "### Task A\n",
    "Predict the following complexes using ColabFold:\n",
    "\n",
    "- Group A: `CcdB:CcdB:CcdA`\n",
    "- Group B: `CcdB:CcdB`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe22ff-6f27-4ee0-a6fc-b170afa73f50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ColabFold job parameters\n",
    "\n",
    "### About CSV input format\n",
    "ColabFold supports input files in CSV format where the `:` symbol allows you to create multimers.\n",
    "\n",
    "```csv\n",
    "id,sequence\n",
    "monomer_A,DSYQLLKAYDVNISGL\n",
    "dimer_A_A,DSYQLLKAYDVNISGL:DSYQLLKAYDVNISGL\n",
    "complex_A_A_B,DSYQLLKAYDVNISGL:DSYQLLKAYDVNISGL:LKAYDVNISGL\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd147a2-bcd0-4cd6-bffe-d975918cbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "job_name         = \"structbio2022_template\"\n",
    "input_name       = \"default_input.csv\"\n",
    "output_path      = \"structbio2022_template\"\n",
    "prediction_input = \"\"\"id,sequence\n",
    "ccda_ccdA,MKQRITVTVDSDSYQLLKAYDVNISGLVSTTMQNEARRLRAERWKAENQEGMAEVARFIEMNGSFADENRDW:MKQRITVTVDSDSYQLLKAYDVNISGLVSTTMQNEARRLRAERWKAENQEGMAEVARFIEMNGSFADENRDW\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524492c-2028-486a-8d00-5e3b0af41aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./input\n",
    "!mkdir -p ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe10ec-f25c-4154-b552-5184847fc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_path = os.path.abspath(os.path.join('./input', input_name))\n",
    "\n",
    "print(\"Saving input file in\", input_path)\n",
    "\n",
    "with open(input_path, \"w\") as file_handler:\n",
    "    file_handler.write(prediction_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5723cf1-9630-43b9-b3fc-487df5209044",
   "metadata": {
    "tags": []
   },
   "source": [
    "### About Slurm jobs in Hydra\n",
    "\n",
    "Slurm provides a complete toolbox to manage and control your jobs. Some of them carry out common tasks: \n",
    "\n",
    "- submitting job scripts to the queue (`sbatch`)\n",
    "- printing information about the queue (`mysqueue`)\n",
    "\n",
    "Jobs are descripted using Bash files with these header options:\n",
    "\n",
    "- `--job-name=job_name` : Set job name to job_name\n",
    "- `--time=DD-HH:MM:SS`: Define the time limit\n",
    "- `--mail-type=BEGIN|END|FAIL|REQUEUE|ALL`: Conditions for sending alerts by email\n",
    "- `--partition=cluster_type`: Request a cluster\n",
    "\n",
    "\n",
    "More information: https://hpc.vub.be/docs/job-submission/\n",
    "\n",
    "### About Singularity\n",
    "Usually, Hydra provides the software you need in _modules_ configured by VSC staff. For instance, Alphafold is available as a module. \n",
    "\n",
    "- To list modules already loaded: `module list`\n",
    "- To search a module: `module spider NAME`\n",
    "- To load a module: `module load NAME`\n",
    "\n",
    "However, when the application is not available yet, you could use containers as a workaround. Containerization is a technique to wrap up a software component inside an isolated and portable file (called image) that contains all the requirements installed and it's ready to be executed independently of the host machine. \n",
    "\n",
    "**Hydra supports Singularity as the official container provider.**\n",
    "\n",
    "Today, we are going to use an image stored in the Singularity cloud: `agdiaz/bio2byte/colabfold:1.3.0`.\n",
    "\n",
    "```bash\n",
    "singularity run \\\n",
    "    --contain \\\n",
    "    --no-home \\\n",
    "    --nv \\\n",
    "    --bind $BASE_DIR:/data,$TEMP_DIR:/tmp \\\n",
    "    library://agdiaz/bio2byte/colabfold:1.3.0 /bin/bash -c \"COMMAND\"\n",
    "```\n",
    "\n",
    "It's important to mention the bindings here:\n",
    "\n",
    "- `$BASE_DIR` in our VSC filesystem is mapped to `/data` inside the container.\n",
    "- `$TEMP_DIR` in our VSC filesystem is mapped to `/tmp` inside the container.\n",
    "\n",
    "More about Containers in Hydra: https://docs.vscentrum.be/en/latest/software/singularity.html\n",
    "\n",
    "### About ColabFold\n",
    "\n",
    "This app is configured by different parameters and the last two of them are the input and the output paths.\n",
    "\n",
    "```bash\n",
    "colabfold_batch \\\n",
    "    --save-pair-representations \\\n",
    "    --save-single-representations \\\n",
    "    --amber \\\n",
    "    --templates \\\n",
    "    --data /data \\\n",
    "    --use-gpu-relax \\\n",
    "    --num-recycle 3 \\\n",
    "    --model-type AlphaFold2-multimer-v2 \\\n",
    "    /data/input/$INPUT_FILE \\\n",
    "    /data/output/{output_path}\n",
    "```\n",
    "\n",
    "#### Available parameters\n",
    "\n",
    "```bash\n",
    "positional arguments:\n",
    "  input                 Can be one of the following: Directory with fasta/a3m\n",
    "                        files, a csv/tsv file, a fasta file or an a3m file\n",
    "  results               Directory to write the results to\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --stop-at-score STOP_AT_SCORE\n",
    "                        Compute models until plddt (single chain) or ptmscore\n",
    "                        (complex) > threshold is reached. This can make\n",
    "                        colabfold much faster by only running the first model\n",
    "                        for easy queries.\n",
    "  --stop-at-score-below STOP_AT_SCORE_BELOW\n",
    "                        Stop to compute structures if plddt (single chain) or\n",
    "                        ptmscore (complex) < threshold. This can make\n",
    "                        colabfold much faster by skipping sequences that do\n",
    "                        not generate good scores.\n",
    "  --num-recycle NUM_RECYCLE\n",
    "                        Number of prediction cycles.Increasing recycles can\n",
    "                        improve the quality but slows down the prediction.\n",
    "  --num-ensemble NUM_ENSEMBLE\n",
    "                        Number of ensembles.The trunk of the network is run\n",
    "                        multiple times with different random choices for the\n",
    "                        MSA cluster centers.\n",
    "  --random-seed RANDOM_SEED\n",
    "                        Changing the seed for the random number generator can\n",
    "                        result in different structure predictions.\n",
    "  --num-models {1,2,3,4,5}\n",
    "  --recompile-padding RECOMPILE_PADDING\n",
    "                        Whenever the input length changes, the model needs to\n",
    "                        be recompiled, which is slow. We pad sequences by this\n",
    "                        factor, so we can e.g. compute sequence from length\n",
    "                        100 to 110 without recompiling. The prediction will\n",
    "                        become marginally slower for the longer input, but\n",
    "                        overall performance increases due to not recompiling.\n",
    "                        Set to 1 to disable.\n",
    "  --model-order MODEL_ORDER\n",
    "  --host-url HOST_URL\n",
    "  --data DATA\n",
    "  --msa-mode {MMseqs2 (UniRef+Environmental),MMseqs2 (UniRef only),single_sequence}\n",
    "                        Using an a3m file as input overwrites this option\n",
    "  --model-type {auto,AlphaFold2-ptm,AlphaFold2-multimer-v1,AlphaFold2-multimer-v2}\n",
    "                        predict strucutre/complex using the following\n",
    "                        model.Auto will pick \"AlphaFold2\" (ptm) for structure\n",
    "                        predictions and \"AlphaFold2-multimer-v2\" for\n",
    "                        complexes.\n",
    "  --amber               Use amber for structure refinement\n",
    "  --templates           Use templates from pdb\n",
    "  --custom-template-path CUSTOM_TEMPLATE_PATH\n",
    "                        Directory with pdb files to be used as input\n",
    "  --env\n",
    "  --cpu                 Allow running on the cpu, which is very slow\n",
    "  --rank {auto,plddt,ptmscore,multimer}\n",
    "                        rank models by auto, plddt or ptmscore\n",
    "  --pair-mode {unpaired,paired,unpaired+paired}\n",
    "                        rank models by auto, unpaired, paired, unpaired+paired\n",
    "  --recompile-all-models\n",
    "                        recompile all models instead of just model 1 ane 3\n",
    "  --sort-queries-by {none,length,random}\n",
    "                        sort queries by: none, length, random\n",
    "  --save-single-representations\n",
    "                        saves the single representation embeddings of all\n",
    "                        models\n",
    "  --save-pair-representations\n",
    "                        saves the pair representation embeddings of all models\n",
    "  --training            turn on training mode of the model to activate drop\n",
    "                        outs\n",
    "  --max-msa {512:5120,512:1024,256:512,128:256,64:128,32:64,16:32}\n",
    "                        defines: `max_msa_clusters:max_extra_msa` number of\n",
    "                        sequences to use\n",
    "  --zip                 zip all results into one <jobname>.result.zip and\n",
    "                        delete the original files\n",
    "  --use-gpu-relax       run amber on GPU instead of CPU\n",
    "  --overwrite-existing-results\n",
    "```\n",
    "\n",
    "- Available Colab Notebook: https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb\n",
    "- Official repository: https://github.com/sokrypton/ColabFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ecfbb-27ba-4a6c-b59c-0652e10680bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_job = \"\"\"#!/bin/bash \n",
    "# General parameters:\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --time=01:30:00\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "# Resources: (1x Nvidia A100 GPU node, 40GB GPU memory, 1x processor, 16x cpu-cores, 192G cpu RAM)\n",
    "#SBATCH --partition=ampere_gpu\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-node=1\n",
    "#SBATCH --cpus-per-gpu=16\n",
    "#SBATCH --mem-per-cpu=12G\n",
    "#SBATCH --reservation=structbio1\n",
    "\n",
    "# Script:\n",
    "\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Starting job on VUB-HPC's (Hydra) GPU clusters AMPERE_GPU (Nvidia A100)\"\n",
    "\n",
    "BASE_DIR={wrkdir}\n",
    "mkdir -p $BASE_DIR\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Base directory: $BASE_DIR\"\n",
    "\n",
    "mkdir -p $BASE_DIR/input\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Directory for input files: $BASE_DIR/input\"\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Sequences to predict will be read from: $BASE_DIR/input/{input_name}\"\n",
    "\n",
    "mkdir -p $BASE_DIR/output/{output_path}\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Results will be available inside directory: $BASE_DIR/output/{output_path}\"\n",
    "\n",
    "CACHE_DIR=$VSC_SCRATCH/.singularity_cache\n",
    "mkdir -p $CACHE_DIR\n",
    "export SINGULARITY_CACHEDIR=$CACHE_DIR\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Singularity cache directory: $CACHE_DIR\"\n",
    "\n",
    "TEMP_DIR=$VSC_SCRATCH/.singularity_tmp\n",
    "mkdir -p $TEMP_DIR\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Singularity tmp directory: $TEMP_DIR\"\n",
    "\n",
    "mkdir -p $BASE_DIR/data\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - ColabFold will download parameters files inside directory (~6GB): $BASE_DIR/data\"\n",
    "\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - All ready to start execution using Singularity image\"\n",
    "\n",
    "singularity run --contain --no-home --nv --bind $BASE_DIR:/data,$TEMP_DIR:/tmp library://agdiaz/bio2byte/colabfold:1.3.0 /bin/bash -c \"source activate /colabfold_batch/colabfold-conda && colabfold_batch --save-pair-representations --save-single-representations --amber --templates --data /data --use-gpu-relax --num-recycle 3 --model-type AlphaFold2-multimer-v2 /data/input/{input_name} /data/output/{output_path}\"\n",
    "\n",
    "echo \"[ColabFold on Hydra from Singularity image] - $(date --rfc-3339=seconds) - Job finished with success\"\n",
    "\"\"\".format(job_name=job_name, input_name=input_name, output_path=output_path, wrkdir=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82ba56-00da-40bf-88f6-732325bab3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_path = os.path.abspath(job_name + \".sh\")\n",
    "\n",
    "print(\"Saving job file in\", job_path)\n",
    "\n",
    "with open(job_path, \"w\") as file_handler:\n",
    "    file_handler.write(prediction_job)\n",
    "\n",
    "print(prediction_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a9bec-ed59-47bc-b9ed-edb8befb4bcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Job enqueueing\n",
    "\n",
    "The job is submitted using the command `sbatch` followed by the name of our job file. You will receive a job identifier as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526f3d3-344a-4597-bd6f-8004dd52d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(['sbatch', job_path],\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "retcode        = process.poll()\n",
    "\n",
    "print(f\"stdout (exit code={retcode}):\")\n",
    "for line in stdout.decode().split(\"\\n\"):\n",
    "    print(line)\n",
    "\n",
    "print(f\"stderr (exit code={retcode}):\")\n",
    "for line in stderr.decode().split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c54f0a-9ee0-41b6-af70-581fcbb846e4",
   "metadata": {},
   "source": [
    "### Query the queue status\n",
    "The command to run is `mysqueue`. While the job is running you could visualize the SLURM logs inside the `slurm-JOBID.out` file:\n",
    "\n",
    "- Cat command: View the full content of the file. `cat slurm-JOBID.out`\n",
    "- Tail command: View the last lines of the file. `tail -f slurm-JOBID.out` (with `-f` the command will follow the output automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33f3e9-2e87-4a01-9220-eb291ebf3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(['mysqueue'],\n",
    "                     stdout=subprocess.PIPE, \n",
    "                     stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "retcode        = process.poll()\n",
    "\n",
    "print(f\"stdout (exit code={retcode}):\")\n",
    "for line in stdout.decode().split(\"\\n\"):\n",
    "    print(line)\n",
    "\n",
    "print(f\"stderr (exit code={retcode}):\")\n",
    "for line in stderr.decode().split(\"\\n\"):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773893b3-70ee-4a8e-b3bb-33f0c3548caa",
   "metadata": {},
   "source": [
    "We are ready to continue working on the next Jupyter Notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
